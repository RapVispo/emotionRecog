{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "324e410c-4ebe-4169-a6de-fa286d4c5757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openvino in /home/rapvispo/openvino_env/lib/python3.8/site-packages (2022.2.0)\n",
      "Requirement already satisfied: numpy<=1.23.1,>=1.16.6 in /home/rapvispo/openvino_env/lib/python3.8/site-packages (from openvino) (1.23.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aeb951f-9e44-476e-86ec-3e6a630fc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32708768-e0e4-4dfe-a8ea-bd0dbb5d1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install pre-req\n",
    "!pip install pillow numpy -q\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1459c2b-d06e-402a-af3c-f12d95b75f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(imagePath, img_height=299):\n",
    "    # Model input format\n",
    "    n, c, h, w = [1, 3, img_height, img_height]\n",
    "    image = Image.open(imagePath)\n",
    "    image = image.resize((h, w), resample=Image.BILINEAR)\n",
    "\n",
    "    # Normalize to keep data between 0 - 1\n",
    "    image = np.array(image) / 255.0\n",
    "\n",
    "    # Change data layout from HWC to CHW\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    input_image = image.reshape((n, c, h, w))\n",
    "\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cc2562b-d177-4ad2-8521-ce42decec37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_process_image_opencv(imagePath, img_height=299):\n",
    "\n",
    "    \n",
    "    pre = cv.imread(imagePath)\n",
    "  \n",
    "    pre = cv.resize(pre,( 64, 64))\n",
    "\n",
    "    # Normalize to keep data between 0 - 1\n",
    "    pre = np.array(pre) / 255.0\n",
    "\n",
    "    # Change data layout from HWC to CHW\n",
    "    pre = pre.transpose((2,0,1))\n",
    "    pre = pre.reshape((1, 3,img_height, img_height))\n",
    "    \n",
    "    #pre = cv.cvtColor(pre, cv.COLOR_RGB2BGR)\n",
    " \n",
    "\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcf099a1-fd2f-4778-a179-5651a5db4237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.78823529, 0.78823529, 0.79607843, ..., 0.78431373,\n",
       "          0.78431373, 0.78039216],\n",
       "         [0.78823529, 0.79215686, 0.79607843, ..., 0.78431373,\n",
       "          0.77647059, 0.78039216],\n",
       "         [0.78823529, 0.79215686, 0.79607843, ..., 0.78431373,\n",
       "          0.78039216, 0.78039216],\n",
       "         ...,\n",
       "         [0.71764706, 0.40392157, 0.64313725, ..., 0.05882353,\n",
       "          0.15294118, 0.30588235],\n",
       "         [0.74509804, 0.38039216, 0.62352941, ..., 0.05490196,\n",
       "          0.18431373, 0.26666667],\n",
       "         [0.7372549 , 0.38823529, 0.59607843, ..., 0.03921569,\n",
       "          0.04705882, 0.01960784]],\n",
       "\n",
       "        [[0.78431373, 0.78431373, 0.79215686, ..., 0.78039216,\n",
       "          0.78039216, 0.77647059],\n",
       "         [0.78431373, 0.78823529, 0.79215686, ..., 0.78039216,\n",
       "          0.77254902, 0.77647059],\n",
       "         [0.78431373, 0.78823529, 0.79215686, ..., 0.78039216,\n",
       "          0.77647059, 0.77647059],\n",
       "         ...,\n",
       "         [0.71764706, 0.42745098, 0.65098039, ..., 0.07843137,\n",
       "          0.17254902, 0.3254902 ],\n",
       "         [0.74509804, 0.41176471, 0.62745098, ..., 0.08627451,\n",
       "          0.22352941, 0.28627451],\n",
       "         [0.7372549 , 0.41960784, 0.6       , ..., 0.0627451 ,\n",
       "          0.06666667, 0.03921569]],\n",
       "\n",
       "        [[0.76862745, 0.76862745, 0.77647059, ..., 0.76470588,\n",
       "          0.76470588, 0.76078431],\n",
       "         [0.76862745, 0.77254902, 0.77647059, ..., 0.76470588,\n",
       "          0.75686275, 0.76078431],\n",
       "         [0.76862745, 0.77254902, 0.77647059, ..., 0.76470588,\n",
       "          0.76078431, 0.76078431],\n",
       "         ...,\n",
       "         [0.74117647, 0.48627451, 0.68235294, ..., 0.09019608,\n",
       "          0.20784314, 0.35686275],\n",
       "         [0.76470588, 0.46666667, 0.66666667, ..., 0.10196078,\n",
       "          0.2627451 , 0.3254902 ],\n",
       "         [0.76078431, 0.4745098 , 0.63921569, ..., 0.07843137,\n",
       "          0.07843137, 0.05098039]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_file_name = 'angry.jpg'\n",
    "\n",
    "#Pre-process the image and get it ready for inference.\n",
    "input_image , = pre_process_image_opencv(inp_file_name, 64), \n",
    "input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bff1c2f0-7872-4773-8908-ef6be80de9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "\n",
    "model_xml = \"./intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml\"\n",
    "model_bin = \"./intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.bin\"\n",
    "\n",
    "# Load network to the plugin\n",
    "ie = IECore()\n",
    "net = ie.read_network(model=model_xml, weights=model_bin)\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "del net\n",
    "\n",
    "input_layer = next(iter(exec_net.input_info))\n",
    "output_layer = next(iter(exec_net.outputs))\n",
    "\n",
    "\n",
    "# Run the Inference on the Input image...\n",
    "res = exec_net.infer(inputs={input_layer: a})\n",
    "#res = res[output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26f975b6-e4e0-4a4e-8e34-2c98c38aa15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prob_emotion': array([[[[0.40683866]],\n",
       " \n",
       "         [[0.05980929]],\n",
       " \n",
       "         [[0.32826462]],\n",
       " \n",
       "         [[0.13653958]],\n",
       " \n",
       "         [[0.06854788]]]], dtype=float32)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
